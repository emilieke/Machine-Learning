{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "## Homework #6: Hidden Markov Models for speech processing\n",
    "\n",
    "### A. The three Basic Problems for HMMs\n",
    "\n",
    "For convenience, we use the compact notation \n",
    "\n",
    "$$\\lambda=(A, B,  \\pi)$$\n",
    "\n",
    "to indicate the complete parameter set of the model, where $A$ is the state transition probability distribution, $B$ the emission probability distribution (which can be any distribution with parameters $\\Theta$) and $\\pi$ the initial state distribution.\n",
    "\n",
    "### Problem 1: \n",
    "Given the observation sequence $O=O_1, O_2, ..., O_T$, and a model $\\lambda=(A, B,  \\pi)$, how do efficiently compute $P(O|\\lambda)$?\n",
    "Problem 1 is the evaluation problem, namely given a model and a sequence of observations, how do we compute the probability that the observed sequence was produced by the model. To solve this problem we use the **forward-backward algorithm**.\n",
    "\n",
    "### Problem 2: \n",
    "Given the observation sequence $O=O_1, O_2, ..., O_T$, and a model $\\lambda=(A, B,  \\pi)$, how do we choose a corresponding state sequence $Q= q_1, q_2, ..., q_T$?\n",
    "Problem 2 is the one in which we attempt to uncover the hidden part of the model, that is, the \"correct\" state sequence. A formal technique for finding thes best state sequence is based on dynamic programming methods, and is called **the Viterbi algorithm**.\n",
    "\n",
    "\n",
    "### Problem 3: \n",
    "How do we adjust the model parameters $\\lambda=(A, B,  \\pi)$ to maximize the probability of the observation sequence given the model $P(O|\\lambda)$?\n",
    "There is no known way to analytically solve this problem. We can, however, choose $\\lambda=(A, B,  \\pi)$ such that $P(O|\\lambda)$? is locally maximized using an iterative procedure such as the **the Baum-Welch algoritm** (or equivalently th EM algorithm).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Reference**\n",
    "\n",
    "Lawrence R. Rabiner \"A tutorial on hidden Markov models and selected applications in speech recognition\" Proceedings of the IEEE 77.2, 1989"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Problem description\n",
    "\n",
    "The aim of this session is to design a HMM-based speech recogniser.\n",
    "\n",
    "The idea is to design a **word speech recogizer**. For each word of the 7 available words we want to fit a separate N-state HMM. We represent the speech signal of a given word as a **time sequence of coded spectral feature vectors**. For each word, we have a training sequence consisting of 15 repetitions of sequences (by one or more talkers).\n",
    "\n",
    "* The first task is to build individual word models. **This task is done by using the solution to Problem 3** to optimally estimate model parameters for each word model.\n",
    "\n",
    "* **To understand the physical meaning of the model states, we use the solution to Problem 2** to divide each of the word training sequences into states, and then study the properties of the spectral vectors that lead to the observation ocurring in each state.\n",
    "\n",
    "* Finally, once the set of 7 HMMs has been fitted and optimized , **recognition of unknown word is performed using the solution of Problem 1**.\n",
    "\n",
    "The file ``words_db.pickle`` contains 15 instances of 7 different words. ``words_db['signals']`` contains the audio signals at a sampling frequency of 8 KHz, ``words_db['features']`` contains a 6 dimensions frequency feature sequences extracted from the audio signals, and ``words_db['word_labels']`` contains the transcription of the words. \n",
    "\n",
    "Depending on the computer hardware specifications, the signals can be reproduced using the package ``audiolab`` from ``scikits``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Word sequences modeling\n",
    "\n",
    "\n",
    "* Load the file and select the instances of the word ``apple``\n",
    "* Divide the instances of the word ``apple`` into train (5) and test (10)\n",
    "* Train a HMM with Gaussian emission probability and 3 hidden states using the train sequences and evaluate the loglikelihood on the test sequences\n",
    "* Plot the loglikelihood on the test sequences using a number of hidden states from 1 to 10 and comment the obtained results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Word classifier\n",
    "\n",
    "We will now train a different HMM for each word, and the output of the classifier will be the word with higher loglikelihood.\n",
    "* Divide the instances of each word into train (5) and test (10)\n",
    "* Train the HMM's and estimate the classification error on the test instances. Print out the confusion matrix.\n",
    "* Use LOO onto the train instances to select the number of hidden states. Try values from 1 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"default\")\n",
    "\n",
    "with open('words_db.pickle', 'rb') as handle:\n",
    "    words_db = pickle.load(handle, encoding='latin1') \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['orange', 'kiwi', 'apple', 'pineapple', 'banana', 'peach', 'lime']\n"
     ]
    }
   ],
   "source": [
    "# signals\n",
    "signals=words_db['signals']\n",
    "# features\n",
    "features=words_db['features']\n",
    "# words labels\n",
    "labels=words_db['labels']\n",
    "#print the different words\n",
    "words = list(set(labels))\n",
    "print(words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3.2]",
   "language": "python",
   "name": "conda-env-py3.2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
