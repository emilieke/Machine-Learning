{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Abdelmalik Moujahid -- abdelmalik.moujahid@uc3m.es\n",
    "\n",
    "Date: March 29, 2017\n",
    "\n",
    "\n",
    "\n",
    "# Machine Learning\n",
    "\n",
    "## Session #6: Introduction to K-means clustering\n",
    "\n",
    "Suppose we have a data set ${x_1,...,x_N}$ consisting of $N$ observations of a random $D$-dimensional variable $x$. The goal of the clustering algorithm is to partition the data set into some number $K$ of clusters. Intuitively, we might think of a cluster as a set of data points whose inter-point distances are small compared with the distances to points outside of the cluster.\n",
    "\n",
    "\n",
    "To formalize this notion let introduce a set of $D$-dimensional vectors $\\mu_k$, where $k=1,...,K$, in which $\\mu_k$ is a prototype (or centroide) associated with the $k^{th}$ cluster.\n",
    "\n",
    "For each data point $x_n$, we introduce a corresponding set of binary indicator variables $r_{nk}\\in {0,1}$, where $k=1,...,K$ describing which of the $K$ clusters the data point $x_n$ is assigned to.\n",
    "\n",
    "We can now define as objective function, sometimes called a **distortion measure (inertia or within-cluster sum-of-squares)**, given by\n",
    "\n",
    "$$J=\\sum_{i=0}^N\\sum_{k=1}^K r_{nk}||x_i-\\mu_k||$$\n",
    "\n",
    "The goal is to find values of the ${r_{nk}}$ and the ${\\mu_k}$ so as to minimize $J$. We can do this through an iterative procesdure in which each iteration involves two successive steps corresponding to successive optimizations wwith respect to the ${r_{nk}}$ and the ${\\mu_k}$. First we choose some initial values for the $\\mu_k$:\n",
    "\n",
    "* We minimize $J$ with respect to the $r_{nk}$, keeping the $\\mu_k$ fixed\n",
    "* We minimize $J$ with respect to the $\\mu_k$, keeping $r_{nk}$  fixed\n",
    "    \n",
    "This two-stage optimization is then repeated until convergence. These two stages of updating  $r_{nk}$ and updating $\\mu_k$ correspond respectively to the **E** (expectation) and **M** (maximization) steps to the **Expectation Maximization (EM) algorithm**.\n",
    "\n",
    "The optimization of $J$ with respect to $r_{nk}$ is achieved by assigning the $n^{th}$ data point to the closest cluster centre. More formally, this can be expressed as:\n",
    "\n",
    "\\begin{equation}\n",
    "r_{nk} = \\left\\{\n",
    "\\begin{array}{l l}\n",
    "1 & \\quad \\textrm{if $k=$arg min$_j$ $||x_n-\\mu_j||^2$}\\\\\n",
    "0  & \\quad \\textrm{otherwise}\n",
    "\\end{array} \\right.\n",
    "\\end{equation}\n",
    "\n",
    "The objective function $J$ is a quadratic function of $\\mu_k$, and then it can be minimized by setting its derivative with respect to $\\mu_k$ to zero giving:\n",
    "\n",
    "$$\\mu_k=\\frac{\\sum_nr_{nk}x_n}{\\sum_nr_{nk}}$$\n",
    "\n",
    "The denominator in this expression is equal to the number of samples assigned to cluster $k$, and so $\\mu_k$ is just the mean of all of the data points $x_n$ assigned to cluster $k$.\n",
    "\n",
    "The two phases of re-assigning data points to clusters and re-computing the cluster means are repeated in turn until there no further change in the assignments (or until some maximum number of iterations is exceeded).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required packages:\n",
    "\n",
    "    * import numpy as np\n",
    "    * from matplotlib import pyplot as plt\n",
    "    * from sklearn import metrics, datasets\n",
    "    * from sklearn.preprocessing import scale\n",
    "    * from sklearn.decomposition import PCA\n",
    "    * from sklearn.cluster import KMeans\n",
    "    * from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Different tasks\n",
    "\n",
    "\n",
    "* Load the digit dataset\n",
    "* Standardize the data along the feature axis\n",
    "* Print the number of samples, the number of feature and the number of classes or digits.\n",
    "* Define a function that print the first 20 images\n",
    "* Fit the k-means algorithm to data using different initialization strategies (See KMeans function help).\n",
    "* Perform a PCA projection of the data and fit a k-means clustering using only the first 10 components.\n",
    "* As the ground truth is known here, apply different cluster quality metrics to judge the goodness of fit of the cluster labels to the ground truth.\n",
    "\n",
    "    Here you have some metrics:\n",
    "    \n",
    "    * metrics.homogeneity_score(true_labels, estimator.labels_),\n",
    "    * metrics.completeness_score(true_labels, estimator.labels_),\n",
    "    * metrics.v_measure_score(true_labels, estimator.labels_),\n",
    "    * metrics.adjusted_rand_score(true_labels, estimator.labels_),\n",
    "    * metrics.adjusted_mutual_info_score(true_labels,  estimator.labels_),\n",
    "    * metrics.silhouette_score(data,estimator.labels_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3.2]",
   "language": "python",
   "name": "conda-env-py3.2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
